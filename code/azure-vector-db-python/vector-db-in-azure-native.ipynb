{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psycopg2\n",
    "%pip install pymongo\n",
    "%pip install redis\n",
    "%pip install openai\n",
    "%pip install python-dotenv\n",
    "# %pip install langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PostgreSQL\n",
    "\n",
    "#### Pgvector extension on Azure Cosmos DB for PostgreSQL\n",
    "\n",
    "- Database client for accessing Postgre SQL (GUI)\n",
    "- https://www.dbvis.com/\n",
    "\n",
    "1. How to configure vector extension\n",
    "\n",
    "    ```postgresql\n",
    "    SELECT CREATE_EXTENSION('vector');\n",
    "    ```\n",
    "\n",
    "    To disable an extension use drop_extension()\n",
    "\n",
    "1. pgvector introduces 3 new operators that can be used to calculate similarity:\n",
    "\n",
    "    | Operator   | Description           |\n",
    "    |:----------:|:---------------------:|\n",
    "    |<->\t     |Euclidean distance     |\n",
    "    |<#>\t     |negative inner product |\n",
    "    |<=>\t     |cosine distance        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "dotenv_path = os.path.join('.', '.env')\n",
    "load_dotenv(dotenv_path, override=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using native wiring API for postgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, '[1,2,3]')\n",
      "(3, '[5,4,6]')\n",
      "(2, '[4,5,6]')\n",
      "(4, '[3,5,7]')\n",
      "(5, '[7,8,9]')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "\n",
    "# Connect to your PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=os.getenv(\"POSTGRE_HOST\"),\n",
    "    database=os.getenv(\"POSTGRE_DB\"),\n",
    "    user=os.getenv(\"POSTGRE_USER\"),\n",
    "    password=os.getenv(\"POSTGRE_PASSWD\")\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the first query to create the table\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE tblvector(\n",
    "        id bigserial PRIMARY KEY,\n",
    "        embedding vector(3)\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# Execute the second query to insert data into the table\n",
    "cur.execute(\"\"\"\n",
    "    INSERT INTO tblvector (id, embedding) VALUES (1, '[1,2,3]'), (2, '[4,5,6]'), (3, '[5,4,6]'), (4, '[3,5,7]'), (5, '[7,8,9]');\n",
    "\"\"\")\n",
    "\n",
    "# Execute the third query to insert or update data in the table\n",
    "cur.execute(\"\"\"\n",
    "    INSERT INTO tblvector (id, embedding) VALUES (1, '[1,2,3]'), (2, '[4,5,6]')\n",
    "    ON CONFLICT (id) DO UPDATE SET embedding = EXCLUDED.embedding;\n",
    "\"\"\")\n",
    "\n",
    "# Execute the fourth query to delete data from the table\n",
    "# cur.execute(\"\"\"\n",
    "#    DELETE FROM tblvector WHERE id = 1;\n",
    "# \"\"\")\n",
    "\n",
    "# Execute the fifth query to select data from the table\n",
    "cur.execute(\"\"\"\n",
    "    SELECT * FROM tblvector \n",
    "    ORDER BY embedding <-> '[3,1,2]' \n",
    "    LIMIT 5;\n",
    "\"\"\")\n",
    "\n",
    "# Fetch and print the results of the SELECT query\n",
    "results = cur.fetchall()\n",
    "for row in results:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Azure Cosmos DB for MongoDB vCore\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using native wiring API for MongoDB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This option does not seem to support several MongoDB commands for management purposes. It is unclear whether it can create a database through Python code. Based on some trials, I have concluded that vCore does not support the use of `customAction`.\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/vcore/compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import urllib\n",
    "\n",
    "# client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "# db = client[\"test\"]\n",
    "# exampleCollection = db[\"exampleCollection\"]\n",
    "\n",
    "encoded_pwd = urllib.parse.quote(os.getenv(\"MONGODB_PASSWD\"))\n",
    "client = pymongo.MongoClient(os.getenv(\"MONGODB_CONNECTION_STRING\").format(password=encoded_pwd))\n",
    "\n",
    "# Create database if it doesn't exist\n",
    "DB_NAME = os.getenv(\"MONGODB_DB\")\n",
    "print(client.list_database_names())\n",
    "db = client[DB_NAME]\n",
    "if DB_NAME not in client.list_database_names():\n",
    "    # Create a database with 400 RU throughput that can be shared across\n",
    "    # the DB's collections\n",
    "    db.command({\"customAction\": \"CreateDatabase\", \"offerThroughput\": 400})\n",
    "    print(\"Created db '{}' with shared throughput.\\n\".format(DB_NAME))\n",
    "else:\n",
    "    print(\"Using database: '{}'.\\n\".format(DB_NAME))\n",
    "\n",
    "# Create collection if it doesn't exist\n",
    "COLLECTION_NAME = os.getenv(\"MONGODB_COLLECTION\")\n",
    "exampleCollection = db[COLLECTION_NAME]\n",
    "if COLLECTION_NAME not in db.list_collection_names():\n",
    "    # Creates a unsharded collection that uses the DBs shared throughput\n",
    "    db.command(\n",
    "        {\"customAction\": \"CreateCollection\", \"collection\": COLLECTION_NAME}\n",
    "    )\n",
    "    print(\"Created collection '{}'.\\n\".format(COLLECTION_NAME))\n",
    "else:\n",
    "    print(\"Using collection: '{}'.\\n\".format(COLLECTION_NAME))\n",
    "\n",
    "# Create indexes\n",
    "indexes = [\n",
    "    {\n",
    "      'name': 'vectorSearchIndex',\n",
    "      'key': {\n",
    "        \"vectorContent\": \"cosmosSearch\"\n",
    "      },\n",
    "      'cosmosSearchOptions': {\n",
    "        'kind': 'vector-ivf',\n",
    "        'numLists': 100,\n",
    "        'similarity': 'COS',\n",
    "        'dimensions': 3\n",
    "      }\n",
    "    }\n",
    "]\n",
    "\n",
    "db.command(\n",
    "  {\n",
    "    'createIndexes': COLLECTION_NAME,\n",
    "    'indexes': indexes\n",
    "  }\n",
    ")\n",
    "\n",
    "# Insert data\n",
    "exampleCollection.insert_many([\n",
    "  {'name': \"Eugenia Lopez\", 'bio': \"Eugenia is the CEO of AdvenureWorks.\", 'vectorContent': [0.51, 0.12, 0.23]},\n",
    "  {'name': \"Cameron Baker\", 'bio': \"Cameron Baker CFO of AdvenureWorks.\", 'vectorContent': [0.55, 0.89, 0.44]},\n",
    "  {'name': \"Jessie Irwin\", 'bio': \"Jessie Irwin is the former CEO of AdventureWorks and now the director of the Our Planet initiative.\", 'vectorContent': [0.13, 0.92, 0.85]},\n",
    "  {'name': \"Rory Nguyen\", 'bio': \"Rory Nguyen is the founder of AdventureWorks and the president of the Our Planet initiative.\", 'vectorContent': [0.91, 0.76, 0.83]},\n",
    "])\n",
    "\n",
    "# Query data\n",
    "queryVector = [0.52, 0.28, 0.12]\n",
    "exampleCollection.aggregate([\n",
    "  {\n",
    "    '$search': {\n",
    "      \"cosmosSearch\": {\n",
    "        \"vector\": queryVector,\n",
    "        \"path\": \"vectorContent\",\n",
    "        \"k\": 2\n",
    "      },\n",
    "    \"returnStoredSource\": True\n",
    "    }\n",
    "  }\n",
    "])\n",
    "\n",
    "# Get metadata\n",
    "exampleCollection.index_information()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cmd\n",
    "OperationFailure: Command CreateDatabase not supported., full error: {'ok': 0.0, 'errmsg': 'Command CreateDatabase not supported.', 'code': 115, 'codeName': 'CommandNotSupported'}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Azure Cognitive Search\n",
    "\n",
    "#### Vector search (private preview) - Azure Cognitive Search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Connect to [Azure SDK Python Dev Feed](https://dev.azure.com/azure-sdk/public/_artifacts/feed/azure-sdk-for-python/connect/pip) to use the alpha version of the azure-search-documents pip package.\n",
    "  - [Download Python](https://www.python.org/downloads/)\n",
    "  - Update Pip: `python -m pip install --upgrade pip`\n",
    "  - Install the keyring `pip install keyring artifacts-keyring`\n",
    "  - If you're using Linux, ensure you've installed the [prerequisites](https://pypi.org/project/artifacts-keyring/), which are required for artifacts-keyring.\n",
    "  - Add a `pip.ini` (Windows) or `pip.conf` (Mac/Linux) file to your virtualenv or where Python is located on your machine:\n",
    "  ```plaintext\n",
    "  [global]\n",
    "  index-url=https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/\n",
    "  ```\n",
    "  - For example, on my machine, I placed mine in the following directory: `%AppData%\\pip\\pip.ini`\n",
    "  - **Note**: Be sure you don't save it as a `.txt` file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The command to check location of `pip.ini` in Windows.\n",
    "\n",
    "    - `pip config -v list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-search-documents==11.4.0a20230509004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install keyring artifacts-keyring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries  \n",
    "import os  \n",
    "import json  \n",
    "import openai  \n",
    "from dotenv import load_dotenv  \n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.models import Vector  \n",
    "from azure.search.documents.indexes.models import (  \n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    PrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SearchField,  \n",
    "    SemanticSettings,  \n",
    "    VectorSearch,  \n",
    "    VectorSearchAlgorithmConfiguration,  \n",
    ")  \n",
    "  \n",
    "# Configure environment variables  \n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")  \n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")  \n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")  \n",
    "openai.api_type = \"azure\"  \n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  \n",
    "openai.api_base = os.getenv(\"OPENAI_ENDPOINT\")  \n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")  \n",
    "credential = AzureKeyCredential(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Document Embeddings using OpenAI Ada 002\n",
    "\n",
    "# Read the text-sample.json\n",
    "sample_path = os.path.join('.', 'text-sample.json')\n",
    "with open(sample_path, 'r', encoding='utf-8') as file:\n",
    "    input_data = json.load(file)\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "# Note: \"engine\" should be set to the deployment name you chose when you deployed the text-embedding-ada-002 model\n",
    "def generate_embeddings(text):\n",
    "    response = openai.Embedding.create(\n",
    "        input=text, engine=os.getenv('OPENAI_DEPLOYMENT_NAME'))\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Generate embeddings for title and content fields\n",
    "for item in input_data:\n",
    "    title = item['title']\n",
    "    content = item['content']\n",
    "    title_embeddings = generate_embeddings(title)\n",
    "    content_embeddings = generate_embeddings(content)\n",
    "    item['titleVector'] = title_embeddings\n",
    "    item['contentVector'] = content_embeddings\n",
    "    item['@search.action'] = 'upload'\n",
    "\n",
    "# Output embeddings to docVectors.json file\n",
    "sample_output_path = os.path.join('.', 'text-sample-output-vector.json')\n",
    "with open(sample_output_path, \"w\") as f:\n",
    "    json.dump(input_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " demoindex created\n"
     ]
    }
   ],
   "source": [
    "# Create a search index\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String,\n",
    "                    searchable=True, retrievable=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String,\n",
    "                    searchable=True, retrievable=True),\n",
    "    SearchableField(name=\"category\", type=SearchFieldDataType.String,\n",
    "                    filterable=True, searchable=True, retrievable=True),\n",
    "    SearchField(name=\"titleVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, dimensions=1536, vector_search_configuration=\"my-vector-config\"),\n",
    "    SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, dimensions=1536, vector_search_configuration=\"my-vector-config\"),\n",
    "]\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    algorithm_configurations=[\n",
    "        VectorSearchAlgorithmConfiguration(\n",
    "            name=\"my-vector-config\",\n",
    "            kind=\"hnsw\",\n",
    "            hnsw_parameters={\n",
    "                \"m\": 4,\n",
    "                \"efConstruction\": 400,\n",
    "                \"efSearch\": 1000,\n",
    "                \"metric\": \"cosine\"\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"title\"),\n",
    "        prioritized_keywords_fields=[SemanticField(field_name=\"category\")],\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields,\n",
    "                    vector_search=vector_search, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 108 documents\n"
     ]
    }
   ],
   "source": [
    "# Upload some documents to the index\n",
    "with open(sample_output_path, 'r') as file:  \n",
    "    documents = json.load(file)  \n",
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)  \n",
    "print(f\"Uploaded {len(documents)} documents\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Azure Cognitive Services\n",
      "Content: Azure Cognitive Services are a set of AI services that enable you to build intelligent applications with powerful algorithms using just a few lines of code. These services cover a wide range of capabilities, including vision, speech, language, knowledge, and search. They are designed to be easy to use and integrate into your applications. Cognitive Services are fully managed, scalable, and continuously improved by Microsoft. It allows developers to create AI-powered solutions without deep expertise in machine learning.\n",
      "Category: AI + Machine Learning\n",
      "\n",
      "Title: Azure Batch AI\n",
      "Content: Azure Batch AI is a fully managed, AI-powered service that enables you to run distributed training and inferencing workloads for your machine learning models at scale. It provides features like automatic scaling, job scheduling, and integration with popular deep learning frameworks, such as TensorFlow, PyTorch, and Caffe. Batch AI supports various platforms, such as .NET, Java, Node.js, and Python. You can use Azure Batch AI to train and deploy your machine learning models, improve your decision-making, and gain insights into your data. It also integrates with other Azure services, such as Azure Machine Learning and Azure Storage.\n",
      "Category: AI + Machine Learning\n",
      "\n",
      "Title: Azure Cognitive Services\n",
      "Content: Azure Cognitive Services is a collection of AI services and APIs that enable you to build intelligent applications using pre-built models and algorithms. It provides features like computer vision, speech recognition, and natural language processing. Cognitive Services supports various platforms, such as .NET, Java, Node.js, and Python. You can use Azure Cognitive Services to build chatbots, analyze images and videos, and process and understand text. It also integrates with other Azure services, such as Azure Machine Learning and Azure Cognitive Search.\n",
      "Category: AI + Machine Learning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pure Vector Search\n",
    "query = \"tools for software development using AI\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))  \n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=\"\",  \n",
    "    vector=Vector(value=generate_embeddings(query), k=3, fields=\"contentVector\"),  \n",
    "    select=[\"title\", \"content\", \"category\"] \n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Azure Cache for Redis\n",
    "\n",
    "#### Azure Cache for Redis Enterprise\n",
    "\n",
    "Most expensive vector database option in Azure. Redis Enterprise only supports limited regions.\n",
    "\n",
    "https://redis.readthedocs.io/en/latest/examples/search_vector_similarity_examples.html\n",
    "\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/redis/getting-started-with-redis-and-openai.ipynb\n",
    "\n",
    "**Note**: Modules must be enabled at the time you create an `Azure Cache for Redis` instance. Must enable `RedisSearch` Module at the time of creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ping returned : True\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import numpy as np\n",
    "from redis.commands.search.field import TagField, VectorField\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "from redis.commands.search.query import Query\n",
    "\n",
    "# r = redis.Redis(host=\"localhost\", port=6379)\n",
    "host_name = os.getenv(\"REDIS_HOST\")\n",
    "access_key = os.getenv(\"REDIS_ACCESS_KEY\")\n",
    "\n",
    "r = redis.StrictRedis(host=host_name, port=10000,\n",
    "                      password=access_key, ssl=True)\n",
    "\n",
    "result = r.ping()\n",
    "print(\"Ping returned : \" + str(result))\n",
    "\n",
    "INDEX_NAME = os.getenv(\"REDIS_INDEX_NAME\")  # Vector Index Name\n",
    "DOC_PREFIX = os.getenv(\"REDIS_DOC_PREFIX\")  # RediSearch Key Prefix for the Index\n",
    "\n",
    "def create_index(vector_dimensions: int):\n",
    "    try:\n",
    "        # check to see if index exists\n",
    "        r.ft(INDEX_NAME).info()\n",
    "        print(\"Index already exists!\")\n",
    "    except:\n",
    "        # schema\n",
    "        schema = (\n",
    "            TagField(\"tag\"),                       # Tag Field Name\n",
    "            VectorField(\"vector\",                  # Vector Field Name\n",
    "                \"FLAT\", {                          # Vector Index Type: FLAT or HNSW\n",
    "                    \"TYPE\": \"FLOAT32\",             # FLOAT32 or FLOAT64\n",
    "                    \"DIM\": vector_dimensions,      # Number of Vector Dimensions\n",
    "                    \"DISTANCE_METRIC\": \"COSINE\",   # Vector Search Distance Metric\n",
    "                }\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # index Definition\n",
    "        definition = IndexDefinition(prefix=[DOC_PREFIX], index_type=IndexType.HASH)\n",
    "\n",
    "        # create Index\n",
    "        r.ft(INDEX_NAME).create_index(fields=schema, definition=definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vector dimensions\n",
    "VECTOR_DIMENSIONS = 1536\n",
    "\n",
    "# create the index\n",
    "create_index(vector_dimensions=VECTOR_DIMENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Create Embeddings with OpenAI text-embedding-ada-002\n",
    "# https://openai.com/blog/new-and-improved-embedding-model\n",
    "\n",
    "texts = [\n",
    "    \"Today is a really great day!\",\n",
    "    \"The dog next door barks really loudly.\",\n",
    "    \"My cat escaped and got out before I could close the door.\",\n",
    "    \"It's supposed to rain and thunder tomorrow.\"\n",
    "]\n",
    "\n",
    "response = openai.Embedding.create(input=texts, engine=os.getenv('OPENAI_DEPLOYMENT_NAME'))\n",
    "embeddings = np.array([r[\"embedding\"] for r in response[\"data\"]], dtype=np.float32)\n",
    "\n",
    "# Write to Redis\n",
    "pipe = r.pipeline()\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    pipe.hset(f\"doc:{i}\", mapping = {\n",
    "        \"vector\": embedding.tobytes(),\n",
    "        \"content\": texts[i],\n",
    "        \"tag\": \"openai\"\n",
    "    })\n",
    "res = pipe.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"animals\"\n",
    "\n",
    "# create query embedding\n",
    "response = openai.Embedding.create(input=[text], engine=os.getenv('OPENAI_DEPLOYMENT_NAME'))\n",
    "query_embedding = np.array([r[\"embedding\"] for r in response[\"data\"]], dtype=np.float32)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for similar documents that have the openai tag\n",
    "query = (\n",
    "    Query(\"(@tag:{ openai })=>[KNN 2 @vector $vec as score]\")\n",
    "     .sort_by(\"score\")\n",
    "     .return_fields(\"content\", \"tag\", \"score\")\n",
    "     .paging(0, 2)\n",
    "     .dialect(2)\n",
    ")\n",
    "\n",
    "query_params = {\"vec\": query_embedding.tobytes()}\n",
    "r.ft(INDEX_NAME).search(query, query_params).docs\n",
    "\n",
    "# the two pieces of content related to animals are returned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
